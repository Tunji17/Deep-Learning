{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine translation with RNNs\n",
    "\n",
    "In this lab, we use RNNs for sequence to sequence (seq2seq) modeling. This consists in producing one sequence of data from another, of possibly different lengths (it is one example of *many-to-many* RNNs you have studied during lectures). More specifically, we work on the *machine translation* task: the goal is to automatically translate a sentence from one language to another. These are respectively called the *source* and *target* languages.\n",
    "\n",
    "<center><a href=\"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\">\n",
    "    <img src=\"https://pytorch.org/tutorials/_images/seq2seq.png\" width=\"500\"></a></center>\n",
    "\n",
    "**Note**: This notebook is based on [this tutorial](https://github.com/bentrevett/pytorch-seq2seq), which you are strongly encouraged to check as it goes into more details about seq2seq models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import datasets\n",
    "import spacy\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function for the network's parameters\n",
    "def init_params(m, seed=0):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data, generator=torch.manual_seed(seed))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight.data, generator=torch.manual_seed(seed))\n",
    "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU) or isinstance(m, nn.RNN):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                nn.init.orthogonal_(param.data, generator=torch.manual_seed(seed))\n",
    "            else:\n",
    "                nn.init.normal_(param.data, generator=torch.manual_seed(seed))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset path - If needed, you can change it HERE but NOWHERE ELSE in the notebook!\n",
    "data_dir = '../datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and preprocessing\n",
    "\n",
    "In this lab we use the Multi30k dataset, which contains sentences in German and English. The preprocessing is the same as in the previous lab (tokenization, vocabularies, numericalization, etc.), except we apply it to both textual entries (sentences in German and English). As a result, the processed dataset contains the following entries:\n",
    "- `\"en\"` and `\"de\"`: the sentences (in English and German),\n",
    "- `\"en_tokens\"` and `\"de_tokens\"`: the tokenized sentences,\n",
    "- `\"en_ids\"` and `\"de_ids\"`: the numericalized sentences (sequences of integers).\n",
    "\n",
    "We also create a vocabulary for each language (`en_vocab` and `de_vocab`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the German and English specific NLP pipelines\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "de_nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Load (eventually download) the dataset\n",
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\", cache_dir=data_dir)\n",
    "train_data, valid_data, test_data = (dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_example(example, en_nlp, de_nlp, lower=True, sos_token='<sos>', eos_token='<eos>'):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n",
    "    \n",
    "train_data = train_data.map(tokenize_example, fn_kwargs={\"en_nlp\": en_nlp,\"de_nlp\": de_nlp})\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs={\"en_nlp\": en_nlp,\"de_nlp\": de_nlp})\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs={\"en_nlp\": en_nlp,\"de_nlp\": de_nlp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabularies\n",
    "min_freq = 2\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"],\n",
    ")\n",
    "de_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"],\n",
    ")\n",
    "en_vocab.set_default_index(0)\n",
    "de_vocab.set_default_index(0)\n",
    "\n",
    "# Numericalization\n",
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs={\"en_vocab\": en_vocab, \"de_vocab\": de_vocab})\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs={\"en_vocab\": en_vocab, \"de_vocab\": de_vocab})\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs={\"en_vocab\": en_vocab, \"de_vocab\": de_vocab})\n",
    "\n",
    "# Type (torch tensors)\n",
    "train_data = train_data.with_format(type=\"torch\", columns=[\"en_ids\", \"de_ids\"], output_all_columns=True)\n",
    "valid_data = valid_data.with_format(type=\"torch\", columns=[\"en_ids\", \"de_ids\"], output_all_columns=True)\n",
    "test_data = test_data.with_format(type=\"torch\", columns=[\"en_ids\", \"de_ids\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of the datasets for faster training\n",
    "train_data = Subset(train_data, range(500))\n",
    "valid_data = Subset(valid_data, range(50))\n",
    "test_data = Subset(test_data, range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset, batch_size, shuffle=False, pad_index=1, seed=0):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "        generator=torch.manual_seed(seed)\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = get_data_loader(train_data, batch_size, shuffle=True)\n",
    "valid_dataloader = get_data_loader(valid_data, batch_size)\n",
    "test_dataloader = get_data_loader(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an example batch\n",
    "example_batch = next(iter(train_dataloader))\n",
    "example_batch_src, example_batch_trg = example_batch['en_ids'], example_batch['de_ids']\n",
    "print(example_batch_src.shape)\n",
    "print(example_batch_src[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq model\n",
    "\n",
    "We now build the machine translation model. This model is based on two parts:\n",
    "\n",
    "- an *encoder*, which takes as input the sentence in the *source* language and encodes it into a *context* vector. This context vector is sort of a summary of the whole input sentence.\n",
    "- a *decoder*, which takes as input this context vector and sequentially generates a sentence in English. It always starts with the `<sos>` token and uses the context vector to generate the second token. Then, it recursively uses the last produced token and the updated hidden state to generate the next token.\n",
    "\n",
    "<center><a href=\"https://github.com/bentrevett/pytorch-seq2seq\">\n",
    "    <img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq1.png\"></a></center>\n",
    "\n",
    "On this picture, $h_t$ represent the hidden states of the encoder, $z$ is the context vector, and $s_t$ the hidden states of the decoder (remember that LSTMs also have a cell state, but they're not displayed here for brevity). The yellow blocks represent the embedding (+dropout), the purple blocks represent the linear classifier, and the green/blue blocks are the recurrent units.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "First, let us build the encoder. It consists of:\n",
    "\n",
    "- an embedding layer to transform token indices into word vectors.\n",
    "- a dropout layer.\n",
    "- a single-layer LSTM, to learn the context vector.\n",
    "\n",
    "**Note**: For the encoder, we don't need to keep track of all the hidden states ($h_1$, $h_2$,..., $h_T$), we only need the final hidden state (= the context vector). Therefore, we can simply apply our LSTM on the whole sequence, instead of writting a loop explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO: initialize the network (remember to use 'self.' for all attributes / parameters / layers)\n",
    "        # - store the input parameters as class attributes\n",
    "        # - create the embedding layer (transform indices into word vectors)\n",
    "        # - create the dropout layer\n",
    "        # - create the LSTM layer\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        # TO DO: write the forward pass\n",
    "        # - compute the embeddings\n",
    "        # - apply dropout to the word embeddings\n",
    "        # - apply the LSTM layer\n",
    "        # - return the final hidden and cell states\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder parameters\n",
    "input_size = len(en_vocab)\n",
    "emb_size_enc = 32\n",
    "hidden_size = 32\n",
    "n_layers = 2\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 1**</span> Instanciate the encoder and print the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can apply the encoder to 'example_batch_src' to get the context vector (made up of 'enc_hidden' and 'enc_cell')\n",
    "enc_hidden, enc_cell = encoder_lstm(example_batch_src)\n",
    "print(enc_hidden.shape)\n",
    "print(enc_cell.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Now, let's build the decoder. We treat machine translation as a classification task: the decoder tries to predict the probability of all token indices in the output (target) vocabulary from an input token index. This has two implications:\n",
    "\n",
    "- In addition to the embedding, dropout and LSTM layers, the decoder applies an extra linear layer to perform prediction of the probabilities. Therefore, this linear layer goes from a space of size `hidden_size_dec` to a space of size `output_size`, which is the number of tokens in the target vocabulary.\n",
    "- the decoder doesn't process all the sentence at once, but instead it processes tokens one by one, since the input at step $t$ is the word that has been predicted at step $t-1$ (not just the hidden state). Therefore, the input to the decoder has a sequence length of 1 (the recursive calculation over the whole sentence will be done in the full model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size, hidden_size, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO:\n",
    "        # - store the attributes\n",
    "        # - create the embedding, dropout, and LSTM layers (it uses recurrent dropout)\n",
    "        # - create the linear layer (it goes from a space of size 'hidden_size' to 'output_size')\n",
    "        \n",
    "    def forward(self, input_idx, input_hidden, input_cell):\n",
    "        \n",
    "        # TO DO: apply the embedding and dropout layers to compute the embeddings y\n",
    "        \n",
    "        # TO DO: since y has a shape [batch_size, hidden_size], we need to unsqueeze it\n",
    "        # to create an artificial 'seq_length' (=1) dimension\n",
    "\n",
    "        # TO DO:\n",
    "        # - apply the LSTM layer. Unlike the encoder, we need to store all the outputs to predict the target token\n",
    "        # - squeeze 'output' (to remove the useless dimension 'seq_length'=1)\n",
    "        # - apply the linear layer to 'output' in order to predict the probabilities\n",
    "        # - return the predicted probabilities per token, and the hidden / cell states of the LSTM decoder\n",
    "        \n",
    "        \n",
    "        return pred_proba, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder parameters\n",
    "output_size = len(de_vocab)\n",
    "emb_size_dec = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 2**</span> Instanciate the decoder and print the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an artificial numerized input token (input indices) with value '2'\n",
    "# (it corresponds to the 'start of sentence' or '<sos>' token)\n",
    "input_idx = torch.ones(example_batch_src.shape[1]).int() * 2\n",
    "# input_idx = torch.ones(batch_size).long() * 2   # depending on your torch version, it might be 'long()' instead of 'int()'\n",
    "\n",
    "# Apply the decoder to this input token using the output of the encoder (hidden and cell)\n",
    "pred_proba, dec_hidden, dec_cell = decoder_lstm(input_idx, enc_hidden, enc_cell)\n",
    "print(pred_proba.shape)\n",
    "print(dec_hidden.shape)\n",
    "print(dec_cell.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: from the tensor of predicted probabilities ('pred_proba'), get the index with the highest probability\n",
    "\n",
    "# TO DO: for each element in the batch, transform this index back to an actual token \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model\n",
    "\n",
    "Finally, we need to implement the overall model, which takes an input sentence, produces the context vectors using the encoder, and produces the output sentence recursively using the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the encoder, decoder, and the target vocabulary size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.trg_vocab_size = decoder.output_size\n",
    "        \n",
    "    def forward(self, src, trg_len):\n",
    "        \n",
    "        # Create a tensor to store the predicted probabilities from the decoder\n",
    "        batch_size = src.shape[-1]\n",
    "        pred_probas_all = torch.zeros(trg_len, batch_size, self.trg_vocab_size)\n",
    "        \n",
    "        # Assign a probability of 1 to the token corresponding to <sos> for the first element\n",
    "        pred_probas_all[0, :, 2] = 1\n",
    "        \n",
    "        # Initialize the first input to the decoder as the <sos> token (coded by '2' in our vocabulary)\n",
    "        input_idx = torch.ones(batch_size).int() * 2\n",
    "        \n",
    "        # TO DO: apply the encoder to the src sentence and get the last hidden and cell states (=context vectors)\n",
    "        # (these will be used as initial hidden/cell for the decoder)\n",
    "        \n",
    "        # loop over tokens (note that it starts from 1 -not 0- since the very first token is already known (=<sos>))\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            # TO DO:\n",
    "            # - apply the decoder to get the predicted probabilites of the token t (and the updated hidden/cell)\n",
    "            # - store these predicted probabilities in the 'pred_probas_all' tensor\n",
    "            # - get the index corresponding to the highest probability for this token: it will be used as the next input index\n",
    "\n",
    "        \n",
    "        return pred_probas_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 3**</span> Instanciate and initialize an encoder and a decoder, then build the full model, and print its number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Apply the model to the 'example_batch'\n",
    "\n",
    "# TO DO: Get the indices of highest predicted probabilities, and print its shape\n",
    "\n",
    "# TO DO: Take one sample from the batch of predicted indices, and transform it back to tokens (remember the 'de_vocab.lookup_tokens' method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "\n",
    "Now we have our model implemented, we can train it. We provide the evaluation and training with validation functions (they are very similar to what was done in previous labs, up to using [gradient clipping](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) in order to prevent exploding gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seq2seq(model, eval_dataloader, loss_fn):\n",
    "\n",
    "    # Set the model in 'eval' mode (disable dropout layer)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the eval loss\n",
    "    eval_loss = 0\n",
    "\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # loop over batches\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "\n",
    "            # Get the source and target sentence, and the target sentence length (varies from batch to batch)\n",
    "            src, trg = batch['en_ids'], batch['de_ids']\n",
    "            trg_len = trg.shape[0]\n",
    "\n",
    "            # Apply the model\n",
    "            pred_probas = model(src, trg_len)\n",
    "\n",
    "            # Remove the first token (always <sos>) to compute the loss\n",
    "            output_size = pred_probas.shape[-1]\n",
    "            pred_probas = pred_probas[1:]\n",
    "\n",
    "            # Reshape the pred_probas and target so that they have appropriate shapes:\n",
    "            #trg: [(trg len - 1) * batch size]\n",
    "            #output: [(trg len - 1) * batch size, output_size]\n",
    "            pred_probas = pred_probas.view(-1, output_size)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(pred_probas, trg)\n",
    "\n",
    "            # Update the total loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Get the average evaluation loss\n",
    "    eval_loss = eval_loss / len(eval_dataloader)\n",
    "    \n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_val_seq2seq(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, verbose=True):\n",
    "\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    # define the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize lists for storing the training and validation losses over epochs\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Initialize the optimal validation loss at +Inf\n",
    "    val_loss_opt = torch.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = 0\n",
    "    \n",
    "        # Set the model in 'training' mode (ensures all parameters' gradients are computed)\n",
    "        model_tr.train()\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Get the source and target sentence, and the target sentence length (varies from batch to batch)\n",
    "            src, trg = batch['en_ids'], batch['de_ids']\n",
    "            trg_len = trg.shape[0]\n",
    "\n",
    "            # Set the gradients at 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Apply the model\n",
    "            pred_probas = model_tr(src, trg_len)\n",
    "\n",
    "            # Remove the first token (always <sos>) to compute the loss\n",
    "            output_dim = pred_probas.shape[-1]\n",
    "            pred_probas = pred_probas[1:]\n",
    "\n",
    "            # Reshape the pred_probas and target\n",
    "            pred_probas = pred_probas.view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss = loss_fn(pred_probas, trg)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model_tr.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the current epoch loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        # At the end of each epoch, get the average training loss and store it\n",
    "        tr_loss = tr_loss/len(train_dataloader)\n",
    "        train_losses.append(tr_loss)\n",
    "        \n",
    "        # Compute the validation loss and store it\n",
    "        val_loss = evaluate_seq2seq(model_tr, valid_dataloader, loss_fn)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Display the training and validation losses at the current epoch\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Training loss: {:.4f} ; Validation loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, tr_loss, val_loss))\n",
    "            \n",
    "        # Save the current model as optimal only if validation loss decreases\n",
    "        if val_loss<val_loss_opt:\n",
    "            model_opt = copy.deepcopy(model_tr)\n",
    "            val_loss_opt = val_loss\n",
    "                \n",
    "    return model_opt, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# For the loss function, since we treat the problem as a classification task, we use the cross entropy.\n",
    "# We also tell it to ignore the index of the <pad> token for computation speed\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=1)\n",
    "\n",
    "# Training\n",
    "model_lstm_tr, train_losses_lstm, val_losses_lstm = training_val_seq2seq(model_lstm, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU network\n",
    "\n",
    "Let us consider an alternative architecture using GRU instead of LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the GRU encoder, decoder, and full seq2seq model. Most of the previous LSTM-related code can be reused.\n",
    "# Remember that a GRU only has two outputs ('out' and 'hidden'), unlike LSTM which has an additional 'cell'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate and initialize the GRU encoder and decoder, then instanciate the GRU full model, and print its number of parameters\n",
    "encoder_gru = GRUEncoder(input_size, emb_size_enc, hidden_size, n_layers, dropout_rate)\n",
    "encoder_gru.apply(init_params)\n",
    "decoder_gru = GRUDecoder(output_size, emb_size_dec, hidden_size, n_layers, dropout_rate)\n",
    "decoder_gru.apply(init_params)\n",
    "model_gru = GRUSeq2Seq(encoder_gru, decoder_gru)\n",
    "print('Number of parameters in the GRU model:', sum(p.numel() for p in model_gru.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: pass the example batch to the GRU model and print the size of the output ('pred_probas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: train the GRU model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the LSTM and GRU models\n",
    "\n",
    "<span style=\"color:red\">**Exercice 4**</span> Compare the two architectures: for each one, print the number of parameters, plot the training/validation loss, and compute the test loss. Based on these, which network would you recommend to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Both models yield very similar results in terms of training behavior (convergence and validation monitoring), as well as a similar test performance and number of parameters. For this specific scenario, GRU might be preferred, since it yields a slightly lower test loss, has less parameters, and does not require to keep track of the additional `cell` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
