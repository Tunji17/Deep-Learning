{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs with Attention\n",
    "\n",
    "In lab 8, we have implemented a seq2seq model based on the encoder-decoder structure, which operates as follows:\n",
    "\n",
    "- The input sentence (source) is passed as input to the encoder, which compresses the information it contains into a *context vector* (= the final hidden/cell state of the encoder).\n",
    "- This context vector is then passed to the decoder, which sequentially decodes and updates it and produces the translated output sentence (target).\n",
    "\n",
    "However, there might be a limitation in this process: at a given step of decoding, it might be preferrable to have access to *all* the hidden states from the encoder rather than a single hidden state. This would allow to know which parts of the input sentence are the most relevent to generate the current word in the output sentence.\n",
    "\n",
    "This can be implemented using a mechanism called **attention**, which is the topic of this lab.\n",
    "\n",
    "Note: As in the previous lab, we'll talk about context and attention *vectors*. In practice, these are not vectors but tensors, since we operate on batches of data. Nevertheless we'll talk about *vectors* since it is consistent with the underlying theory.\n",
    "\n",
    "<center><a href=\"https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b\">\n",
    "    <img src=\"https://miro.medium.com/max/2000/1*FP3zFjdFhNUWEJ9hxeIYOA.png\" width=\"800\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import datasets\n",
    "import spacy\n",
    "import torchtext;\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function for the network's parameters\n",
    "def init_params(m, seed=0):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data, generator=torch.manual_seed(seed))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight.data, generator=torch.manual_seed(seed))\n",
    "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU) or isinstance(m, nn.RNN):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                nn.init.orthogonal_(param.data, generator=torch.manual_seed(seed))\n",
    "            else:\n",
    "                nn.init.normal_(param.data, generator=torch.manual_seed(seed))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset and pretrained model path - If needed, you can change it HERE but NOWHERE ELSE in the notebook!\n",
    "data_dir = '../datasets/'\n",
    "pretrained_model_path = './model_attention_large.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and preprocessing\n",
    "\n",
    "The dataset and preprocessing are the same as in lab 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NLP pipelines\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "de_nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Load (eventually download) the dataset\n",
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\", cache_dir=data_dir)\n",
    "train_data, valid_data, test_data = (dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_example(example, en_nlp, de_nlp, lower=True, sos_token='<sos>', eos_token='<eos>'):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n",
    "    \n",
    "train_data = train_data.map(tokenize_example, fn_kwargs={\"en_nlp\": en_nlp,\"de_nlp\": de_nlp})\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs={\"en_nlp\": en_nlp,\"de_nlp\": de_nlp})\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs={\"en_nlp\": en_nlp,\"de_nlp\": de_nlp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabularies\n",
    "min_freq = 2\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"],\n",
    ")\n",
    "de_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"],\n",
    ")\n",
    "en_vocab.set_default_index(0)\n",
    "de_vocab.set_default_index(0)\n",
    "\n",
    "# Numericalization\n",
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs={\"en_vocab\": en_vocab, \"de_vocab\": de_vocab})\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs={\"en_vocab\": en_vocab, \"de_vocab\": de_vocab})\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs={\"en_vocab\": en_vocab, \"de_vocab\": de_vocab})\n",
    "\n",
    "# Type (torch tensors)\n",
    "train_data = train_data.with_format(type=\"torch\", columns=[\"en_ids\", \"de_ids\"], output_all_columns=True)\n",
    "valid_data = valid_data.with_format(type=\"torch\", columns=[\"en_ids\", \"de_ids\"], output_all_columns=True)\n",
    "test_data = test_data.with_format(type=\"torch\", columns=[\"en_ids\", \"de_ids\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of the datasets for faster training\n",
    "train_data = Subset(train_data, range(500))\n",
    "valid_data = Subset(valid_data, range(50))\n",
    "test_data = Subset(test_data, range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset, batch_size, shuffle=False, pad_index=1, seed=0):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "        generator=torch.manual_seed(seed)\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = get_data_loader(train_data, batch_size, shuffle=True)\n",
    "valid_dataloader = get_data_loader(valid_data, batch_size)\n",
    "test_dataloader = get_data_loader(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 32])\n",
      "tensor([   2,   48,   30,   17,  400,   51,    4,    0,    8,    4,  331, 2353,\n",
      "           5,    3,    1,    1,    1,    1,    1,    1,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "# Get an example batch\n",
    "example_batch = next(iter(train_dataloader))\n",
    "example_batch_src, example_batch_trg = example_batch['en_ids'], example_batch['de_ids']\n",
    "print(example_batch_src.shape)\n",
    "print(example_batch_src[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the parameters of the network (small model for speed)\n",
    "input_size = len(en_vocab)\n",
    "output_size = len(de_vocab)\n",
    "emb_size_enc = 32\n",
    "emb_size_dec = 32\n",
    "hidden_size = 50\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU encoder\n",
    "\n",
    "We use a similar encoder to the previous lab. The main difference is that it needs to output the whole sequence of hidden states (not only the final hidden state / context vector) because we will use it in the attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size_enc, hidden_size, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # TO DO:\n",
    "        # - store the input parameters as class attributes\n",
    "        # - create the embedding, dropout, and GRU layers (the GRU layer does not use recurrent dropout)\n",
    "        self.input_size = input_size\n",
    "        self.emb_size_enc = emb_size_enc\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(input_size, emb_size_enc)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.gru = nn.GRU(emb_size_enc, hidden_size, n_layers)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        # TO DO:\n",
    "        # - write the forward pass\n",
    "        # - return both outputs of the GRU (call them 'enc_outputs' and 'enc_context')\n",
    "        y = self.embedding_layer(src)\n",
    "        y = self.dropout(y)\n",
    "        enc_outputs, enc_context = self.gru(y)\n",
    "\n",
    "        return enc_outputs, enc_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 1**</span> Instanciate the encoder and print the number of parameters. Apply the encoder to the example batch and print the shapes of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 32, 50])\n",
      "torch.Size([1, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the encoder\n",
    "encoder = Encoder(input_size, emb_size_enc, hidden_size, n_layers, dropout_rate)\n",
    "encoder.apply(init_params)\n",
    "enc_outputs, enc_context = encoder(example_batch_src)\n",
    "print(enc_outputs.shape)\n",
    "print(enc_context.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The attention mechanism\n",
    "\n",
    "We now implement the attention mechanism. Intuitively, the idea behind attention is to identify which word of the source sentence is the most important to generate the current word at decoding.\n",
    "\n",
    "Mathematically, let's note $t'$ the current decoding step, $s_{t'-1}$ the previous hidden state of the decoder, and $H = \\{h_1, h_2,...,h_T \\}$  the set of all encoder outputs at the last layer. The attention vector $a_{t'}$ will therefore be calculated from $s_{t'-1}$ and $H$ through a set of operation with learnable parameters.\n",
    "\n",
    "### An example\n",
    "\n",
    "For the very first target token, the previous hidden state is given by $s_0$ = $z$ (= the context vector). The attention mechanism is illustrated below in this case, and we will consider it as example.\n",
    "\n",
    "<center><a href=\"https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\">\n",
    "    <img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq9.png\" width=\"500\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the context vector of the encoder and treat it as the first hidden state to the decoder\n",
    "dec_hidden = enc_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to assemble / combine the current hidden state of the decoder $s_{t'-1}$ (here, it's simply the context vector) and the encoder outputs into a single tensor. There are many ways to do so (addition, multiplication...), here we simply concatenate them.\n",
    "\n",
    "**Note**: this is easy to implement when the recurrent part uses 1 layer; otherwise the computation would be more involved as we'd need to consider the extra dimension corresponding to the number of layers. We leave that to further exploration.\n",
    "\n",
    "Currently (on this example):\n",
    "- `enc_outputs` has a size of `[src_len, batch_size, hidden_size]`\n",
    "- `dec_hidden` has a size of `[1, batch_size, hidden_size]`\n",
    "\n",
    "We want to concatenate them along the last dimension, so first we need to repeat `dec_hidden` along the first dimension `src_len` times. Then, we can perform the concatenation. Finally, we permute the combined tensor so it has a size of `[batch_size, src_len, 2 * hidden_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 22, 100])\n"
     ]
    }
   ],
   "source": [
    "# TO DO: compute this concatenation:\n",
    "# - repeat it using the 'repeat' function (check the doc!)\n",
    "# - concatenate the features using the 'cat' function\n",
    "# - permute the dimensions so the resulting combined input has shape [batch_size, src_len, 2*hidden_size]\n",
    "combined_input = torch.cat((dec_hidden.repeat(enc_outputs.shape[0], 1, 1), enc_outputs), dim=2)\n",
    "combined_input = combined_input.permute(1, 0, 2)\n",
    "print(combined_input.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write a network to compute attention from this combined tensor: $a = f(c)$. This network depends on two parameters values, which are:\n",
    " - `input_size_att`, which is the length of the input combined tensor, thus it's equal to `2 * hidden_size`.\n",
    " - `hidden_size_att`, which corresponds to the intermediate length used in computing the attention vector.\n",
    "\n",
    "The architecture of the network is as follows:\n",
    "\n",
    "- a linear layer that goes from size `input_size_att` to `hidden_size_att`\n",
    "- a tanh activation\n",
    "- a second linear layer, that goes from size `hidden_size_att` to `1`, and does not use bias\n",
    "- a softmax layer that acts along `dim=1`, to ensure that the attention vector sums to 1 for every sentence in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - define the attention network parameters ('input_size_att', and 'hidden_size_att'=50)\n",
    "# - write the attention module\n",
    "# - apply it to the combined input tensor 'comb_inputs'\n",
    "# - squeeze the output and print its shape\n",
    "input_size_att = 2*hidden_size\n",
    "hidden_size_att = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention module\n",
    "\n",
    "Now, we can use what we did above on the example to write the full attention module in the general case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size_att, hidden_size_att):\n",
    "        super().__init__()\n",
    "\n",
    "        # TO DO: store the input parameters as attributes and define the network\n",
    "        self.input_size_att = input_size_att\n",
    "        self.hidden_size_att = hidden_size_att\n",
    "\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(input_size_att, hidden_size_att),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size_att, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, dec_hidden, enc_outputs):\n",
    "        # TO DO: write the forward method to compute the attention vector\n",
    "        combined_input = torch.cat((dec_hidden.repeat(enc_outputs.shape[0], 1, 1), enc_outputs), dim=2)\n",
    "        combined_input = combined_input.permute(1, 0, 2)\n",
    "        attention_scores = self.attention_net(combined_input)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 2**</span> Instanciate an attention module, apply it to `dec_hidden` and `enc_outputs`, and print the size of the attention vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention = Attention(input_size_att, hidden_size_att)\n",
    "# attn = attention(dec_hidden, enc_outputs)\n",
    "# print(attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with attention\n",
    "\n",
    "At the decoding step, we can now use the attention vector by applying it to the encoder outputs. This results in the *weighted* vector which is the average of encoder outputs scaled by attention:\n",
    "\n",
    "$$\n",
    "w = \\sum_t a_t \\times h_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Compute the weighted vector.\n",
    "# - you need to expand/unsqueeze the attention vector 'a' so it has size [batch_size, 1, src_len]\n",
    "# - you also need to permute the enc_outputs vector so it has size [batch_size, src_len, hidden_size]\n",
    "# - perform multiplication using 'torch.bmm' (no need to use a for loop)\n",
    "# - permute the result so the weighted vector has size [1, batch_size, hidden_size]\n",
    "# a = attn.unsqueeze(1)\n",
    "# # Remove last dimension of a\n",
    "# a = a.squeeze(-1)\n",
    "# print(\"Shape of a:\", a.shape)\n",
    "# enc_outputs = enc_outputs.permute(1, 0, 2)\n",
    "# print(\"Shape of enc_outputs:\", enc_outputs.shape)\n",
    "# multiplicated = torch.bmm(a, enc_outputs)\n",
    "# weighted_vector = multiplicated.permute(1, 0, 2)\n",
    "# print(\"Shape of weighted_vector:\", weighted_vector.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to take this weighted vector into account when using the RNN (which here is a GRU). We remind that without attention, the recurrent computation is simply $s_{t'} = \\text{GRU}(y_{t'}, s_{t'-1})$ where $y_{t'}$ is the embedding after dropout. When using attention, the formula becomes:\n",
    "\n",
    "$$\n",
    "s_{t'} = \\text{GRU}([y_{t'}, w], s_{t'-1})\n",
    "$$\n",
    "\n",
    "This means the input of the GRU in the decoder is obtained by concatenating the weighted vector with the embedding after dropout $y_{t'}$.\n",
    "\n",
    "**Note**: using this concatenation changes the size of the GRU input: the input size should no longer be `emb_size_dec`, but `emb_size_dec + hidden_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size_dec, hidden_size, hidden_size_att, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\t# Missing key(s) in state_dict:  \"decoder.attention.linear_layer.weight\", \"decoder.attention.linear_layer.bias\", \"decoder.attention.attention.weight\", \"decoder.fc.weight\", \"decoder.fc.bias\".\n",
    "\t# Unexpected key(s) in state_dict:  \"decoder.linear_layer.weight\", \"decoder.linear_layer.bias\", \"decoder.attention.attention_net.0.weight\", \"decoder.attention.attention_net.0.bias\", \"decoder.attention.attention_net.2.weight\".\n",
    "\n",
    "        # Store input parameters as class attributes\n",
    "        self.output_size = output_size\n",
    "        self.emb_size_dec = emb_size_dec\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_att = hidden_size_att\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Create the decoder layers and the attention module\n",
    "        self.embedding_layer = nn.Embedding(output_size, emb_size_dec)\n",
    "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "        self.gru = nn.GRU(hidden_size + emb_size_dec, hidden_size, n_layers)\n",
    "        self.attention = Attention(2 * hidden_size, hidden_size_att)\n",
    "        self.linear_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_idx, input_hidden, enc_outputs):\n",
    "        # Get the embeddings for the input token (same as in the previous lab)\n",
    "        y = self.dropout_layer(self.embedding_layer(input_idx))\n",
    "        y = y.unsqueeze(0)\n",
    "\n",
    "        # Compute the attention vector\n",
    "        a = self.attention(input_hidden, enc_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        a = a.squeeze(-1)\n",
    "        \n",
    "        # Compute the weighted vector\n",
    "        enc_outputs = enc_outputs.permute(1, 0, 2)\n",
    "        weighted_vector = torch.bmm(a, enc_outputs).permute(1, 0, 2)\n",
    "        \n",
    "        # Concatenate the embedding (after dropout) and the weighted vector\n",
    "        combined_input = torch.cat((y, weighted_vector), dim=2)\n",
    "\n",
    "        # Apply the GRU layer\n",
    "        output, hidden = self.gru(combined_input, input_hidden)\n",
    "        \n",
    "        # Squeeze the output of the GRU and pass it to the linear layer to have the predicted probabilities\n",
    "        output = self.linear_layer(output.squeeze(0))\n",
    "\n",
    "        return output, hidden, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 3**</span> Instanciate the decoder and print the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7853])\n",
      "torch.Size([1, 32, 50])\n",
      "torch.Size([32, 1, 22])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the decoder\n",
    "decoder = Decoder(output_size, emb_size_dec, hidden_size, hidden_size_att, n_layers, dropout_rate)\n",
    "decoder.apply(init_params)\n",
    "dec_output, dec_hidden, a = decoder(example_batch_trg[0], dec_hidden, enc_outputs)\n",
    "print(dec_output.shape)\n",
    "print(dec_hidden.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 50])\n",
      "torch.Size([1, 32, 50])\n",
      "torch.Size([32, 1, 22])\n"
     ]
    }
   ],
   "source": [
    "# Initialize an input index tensor (corresponds to <sos>)\n",
    "current_batch_size = a.shape[0]\n",
    "input_idx = torch.ones(current_batch_size).int() * 2\n",
    "\n",
    "# Apply the decoder, print the shape of the outputs\n",
    "pred_proba, hidden, a = decoder(input_idx, enc_context, enc_outputs)\n",
    "\n",
    "print(enc_context.shape)\n",
    "print(hidden.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model\n",
    "\n",
    "The full model is the same as in the previous lab. The only difference comes from the fact that we store the attention vector at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.trg_vocab_size = decoder.output_size\n",
    "\n",
    "  def forward(self, src, trg_len):\n",
    "\n",
    "    batch_size = src.shape[-1]\n",
    "    pred_probas_all = torch.zeros(trg_len, batch_size, self.trg_vocab_size)\n",
    "    attention_all = torch.zeros(trg_len, batch_size, src.shape[0])\n",
    "    pred_probas_all[0, :, 2] = 1\n",
    "    input_idx = torch.ones(batch_size).int() * 2\n",
    "    enc_outputs, hidden = self.encoder(src)\n",
    "\n",
    "    for t in range(1, trg_len):\n",
    "      pred_proba_t, hidden, attn = self.decoder(input_idx, hidden, enc_outputs)\n",
    "      pred_probas_all[t, :, :] = pred_proba_t.squeeze(0)\n",
    "      attention_all[t, :, :] = attn.squeeze(1)\n",
    "      input_idx = pred_proba_t.argmax(dim=1)\n",
    "\n",
    "    return pred_probas_all, attention_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate and initialize the encoder and decoder, then instanciate the full model, and print its number of parameters\n",
    "encoder = Encoder(input_size, emb_size_enc, hidden_size, n_layers, dropout_rate)\n",
    "encoder.apply(init_params)\n",
    "decoder = Decoder(output_size, emb_size_dec, hidden_size, hidden_size_att, n_layers, dropout_rate)\n",
    "decoder.apply(init_params)\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 32, 7853])\n",
      "torch.Size([25, 32, 22])\n"
     ]
    }
   ],
   "source": [
    "# Apply it to the example_batch, and print the output shapes\n",
    "trg_len = example_batch_trg.shape[0]\n",
    "pred_probas_all, attention_all = model(example_batch_src, trg_len)\n",
    "print(pred_probas_all.shape)\n",
    "print(attention_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (with validation) and evaluation\n",
    "\n",
    "We provide below the evaluation and training functions, since they're the same as in the previous lab (except we have to handle the fact that the model returns two outputs: `pred_probas_all` and `attention_all`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seq2seq(model, eval_dataloader, loss_fn):\n",
    "    \n",
    "    # Set the model in 'eval' mode (disable dropout layer)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the eval loss\n",
    "    eval_loss = 0\n",
    "\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # loop over batches\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "\n",
    "            # Get the source and target sentence, and the target sentence length (varies from batch to batch)\n",
    "            src, trg = batch['en_ids'], batch['de_ids']\n",
    "            trg_len = trg.shape[0]\n",
    "\n",
    "            # Apply the model\n",
    "            pred_probas, _ = model(src, trg_len)\n",
    "\n",
    "            # Remove the first token (always <sos>) to compute the loss\n",
    "            output_size = pred_probas.shape[-1]\n",
    "            pred_probas = pred_probas[1:]\n",
    "\n",
    "            # Reshape the pred_probas and target so that they have appropriate sizes\n",
    "            pred_probas = pred_probas.view(-1, output_size)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(pred_probas, trg)\n",
    "\n",
    "            # Record the loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Get the average evaluation loss\n",
    "    eval_loss = eval_loss / len(eval_dataloader)\n",
    "    \n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_val_seq2seq(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, verbose=True):\n",
    "    \n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    # define the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize lists for storing the training and validation losses over epochs\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Initialize the optimal validation loss at +Inf\n",
    "    val_loss_opt = torch.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Set the model in 'training' mode (ensures all parameters' gradients are computed)\n",
    "        model_tr.train()\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Get the source and target sentence, and the target sentence length (varies from batch to batch)\n",
    "            src, trg = batch['en_ids'], batch['de_ids']\n",
    "            trg_len = trg.shape[0]\n",
    "\n",
    "            # Set the gradients at 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Apply the model\n",
    "            pred_probas, _ = model_tr(src, trg_len)\n",
    "\n",
    "            # Remove the first token (always <sos>) to compute the loss\n",
    "            output_dim = pred_probas.shape[-1]\n",
    "            pred_probas = pred_probas[1:]\n",
    "\n",
    "            # Reshape the pred_probas and target\n",
    "            pred_probas = pred_probas.view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss = loss_fn(pred_probas, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_tr.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        # At the end of each epoch, get the average training loss and store it\n",
    "        tr_loss = tr_loss/len(train_dataloader)\n",
    "        train_losses.append(tr_loss)\n",
    "        \n",
    "        # Compute the validation loss and store it\n",
    "        val_loss = evaluate_seq2seq(model_tr, valid_dataloader, loss_fn)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Display the training and validation losses at the current epoch\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Training loss: {:.4f} ; Validation loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, tr_loss, val_loss))\n",
    "            \n",
    "        # Save the current model as optimal only if validation loss decreases\n",
    "        if val_loss<val_loss_opt:\n",
    "            model_opt = copy.deepcopy(model_tr)\n",
    "            val_loss_opt = val_loss\n",
    "                \n",
    "    return model_opt, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 4**</span> Train the model, plot the training and validation losses, and compute the test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Training loss: 8.5835 ; Validation loss: 8.0490\n",
      "Epoch [2/15], Training loss: 7.4109 ; Validation loss: 7.0066\n",
      "Epoch [3/15], Training loss: 6.3973 ; Validation loss: 6.2942\n",
      "Epoch [4/15], Training loss: 5.7505 ; Validation loss: 5.9025\n",
      "Epoch [5/15], Training loss: 5.4265 ; Validation loss: 5.7376\n",
      "Epoch [6/15], Training loss: 5.2786 ; Validation loss: 5.6945\n",
      "Epoch [7/15], Training loss: 5.2310 ; Validation loss: 5.6963\n",
      "Epoch [8/15], Training loss: 5.2101 ; Validation loss: 5.7086\n",
      "Epoch [9/15], Training loss: 5.2047 ; Validation loss: 5.7182\n",
      "Epoch [10/15], Training loss: 5.1945 ; Validation loss: 5.7312\n",
      "Epoch [11/15], Training loss: 5.1908 ; Validation loss: 5.7405\n",
      "Epoch [12/15], Training loss: 5.1909 ; Validation loss: 5.7562\n",
      "Epoch [13/15], Training loss: 5.1795 ; Validation loss: 5.7653\n",
      "Epoch [14/15], Training loss: 5.1781 ; Validation loss: 5.7764\n",
      "Epoch [15/15], Training loss: 5.1783 ; Validation loss: 5.7820\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_opt, train_losses, val_losses = training_val_seq2seq(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 5.6257\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss = evaluate_seq2seq(model_opt, test_dataloader, loss_fn)\n",
    "print('Test loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation losses\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspectives\n",
    "\n",
    "Let's quickly check the results on one example from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    sos_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    device='cpu',\n",
    "    output_length=15,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # prepare the input sentence (tokenization, numericalization)\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(sentence)]\n",
    "    en_tokens = [token.lower() for token in en_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    ids = en_vocab.lookup_indices(en_tokens)\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "\n",
    "    # Compute the translation using the model\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(tensor, output_length)\n",
    "        outputs = torch.argmax(outputs, -1)\n",
    "    outputs = [x.item() for x in outputs]\n",
    "    \n",
    "    translated = de_vocab.lookup_tokens(outputs)\n",
    "\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:               A man in an orange hat starring at something.\n",
      "True translation:       Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "Predicted translation:  ['<sos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# Get an example sentence and its translation\n",
    "sentence = test_data[0][\"en\"]\n",
    "true_translation = test_data[0][\"de\"]\n",
    "\n",
    "# Get the true length of the (tokenized) translated sentence\n",
    "trg_len = len([token.text for token in de_nlp.tokenizer(true_translation)])\n",
    "\n",
    "# Translate the sentence\n",
    "pred_translation = translate_sentence(\n",
    "    sentence,\n",
    "    model_opt,\n",
    "    en_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    output_length=trg_len\n",
    ")\n",
    "\n",
    "print('Sentence:              ', sentence)\n",
    "print('True translation:      ', true_translation)\n",
    "print('Predicted translation: ', pred_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on the example above, the results are not satisfactory. This can be explained by a variety of factors, but mainly because we consider a very small subset of the data and because we used a very light network. We provide the pretrained weights of a model with larger hidden and embedding sizes, and trained on the whole dataset (you can reproduce it using the provided parameters and using a batch size of 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.embedding_layer.weight', 'encoder.gru.weight_ih_l0', 'encoder.gru.weight_hh_l0', 'encoder.gru.bias_ih_l0', 'encoder.gru.bias_hh_l0', 'decoder.embedding_layer.weight', 'decoder.gru.weight_ih_l0', 'decoder.gru.weight_hh_l0', 'decoder.gru.bias_ih_l0', 'decoder.gru.bias_hh_l0', 'decoder.linear_layer.weight', 'decoder.linear_layer.bias', 'decoder.attention.attention_net.0.weight', 'decoder.attention.attention_net.0.bias', 'decoder.attention.attention_net.2.weight'])\n",
      "Sentence:               A man in an orange hat starring at something.\n",
      "True translation:       Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "Predicted translation:  ['<sos>', 'ein', 'mann', 'mit', 'einem', 'kopfbedeckung', 'starrt', 'etwas', 'etwas', 'etwas', '.']\n"
     ]
    }
   ],
   "source": [
    "# Instanciate a large model and load the pretrained parameters\n",
    "input_size = len(en_vocab)\n",
    "output_size = len(de_vocab)\n",
    "emb_size_enc = 256\n",
    "emb_size_dec = 256\n",
    "hidden_size = 512\n",
    "hidden_size_att = 512\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5\n",
    "encoder = Encoder(input_size, emb_size_enc, hidden_size, n_layers, dropout_rate)\n",
    "decoder = Decoder(output_size, emb_size_dec, hidden_size, hidden_size_att, n_layers, dropout_rate)\n",
    "model_large = Seq2Seq(encoder, decoder)\n",
    "\n",
    "pretrained_model = torch.load(pretrained_model_path)\n",
    "print(pretrained_model.keys())\n",
    "model_large.load_state_dict(pretrained_model)\n",
    "\n",
    "# Translate the sentence\n",
    "pred_translation = translate_sentence(\n",
    "    sentence,\n",
    "    model_large,\n",
    "    en_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    output_length=trg_len\n",
    ")\n",
    "\n",
    "print('Sentence:              ', sentence)\n",
    "print('True translation:      ', true_translation)\n",
    "print('Predicted translation: ', pred_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are much better with this larger network, although not perfect since both the network structure and training protocol are rather basic. There are many other strategies to improve this task's performance (but also many other text processing / NLP tasks based on RNNs - or not):\n",
    "\n",
    "- using *bi-directionnal* recurrent networks to better account for the whole context of the sentence.\n",
    "- *skip-filtering*, which means feeding each RNN cell with the whole context vector instead of just the previous hidden state (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb)).\n",
    "- *teacher forcing* when training the decoder, which means using the ground truth token as input to each decoding step, instead of the predicted token from the previous step (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)).\n",
    "- *packed padded sentences* with masking, which allows to skip the `<pad>` token in the encoder and save time (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/main/legacy/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb)).\n",
    "- *self-attention*, a key component in [transformers](https://arxiv.org/pdf/1706.03762.pdf), which are state-of-the-art architectures for machine translation (see a simplified version implemented [here](https://github.com/bentrevett/pytorch-seq2seq/blob/main/legacy/6%20-%20Attention%20is%20All%20You%20Need.ipynb)).\n",
    "\n",
    "And many more - research continues to advance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_speech_recognition-qgr9tgLb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
