{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU-Ja8a7_5sf"
      },
      "source": [
        "Author: Oyetunji Abioye"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwOn9fUKLvgN"
      },
      "source": [
        "**LAB1 - NeSy Recommendation System**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP4ysl17LNct"
      },
      "source": [
        "Movie Recommendation System\n",
        "Recommend movies to users based on explicit preferences.\n",
        "\n",
        "We will go through probabilistic uncertainty, vague knowledge, commonsense reasoning, and similarity-based inference. More precisely:\n",
        "\n",
        "* Classical Prolog= Structured rules for explicit knowledge (e.g., “Alice likes Sci-Fi → Recommend Sci-Fi movies”)\n",
        "* Probabilistic Prolog: Uncertain preferences (e.g., “Alice probably likes Sci-Fi with 80% confidence”)\n",
        "* Possibilistic Prolog Vague or imprecise knowledge (e.g., “If Alice likes Sci-Fi, she might like Cyberpunk”)\n",
        "* Commonsense Reasoning\tInterpolation & analogy-based reasoning (e.g., “Alice likes Action and Sci-Fi → Infer she might like Cyberpunk”)\n",
        "* Similarity-Based Reasoning\tVector embeddings for contextual similarity (e.g., “Inception is similar to Interstellar → Recommend Interstellar”)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUf8UH8MYe7"
      },
      "source": [
        "Let's do concept spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzDawH1ELqy3"
      },
      "source": [
        "Let's start with the following step\n",
        "\n",
        "* Step 1: Define the Logical Rules (Prolog)\n",
        "* Step 2: Introduce Probabilistic Reasoning (Probabilistic Prolog)\n",
        "* Step 3: Implement Commonsense Reasoning\n",
        "* Step 4: Implement Similarity-Based Reasoning Using Vector Embeddings\n",
        "* Step 5:\n",
        "    - Integrate real-world datasets (IMDB, MovieLens)\n",
        "    - Enhance user personalization (e.g., feedback loops)\n",
        "    - Reason with Real Embeddings (embed IMDB for example)\n",
        "* Step 3: Handle Vague Knowledge (Possibilistic Prolog)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoJRaQ7B_v68"
      },
      "source": [
        "[Lecture Reference](https://colab.research.google.com/drive/1TmrVRMbc83WaIK27eDfAseiIVRTZ-Cnc?usp=sharing)\n",
        "\n",
        "\n",
        "[Python Prolog Documentation](https://github.com/yuce/pyswip)\n",
        "\n",
        "[Python Problog Documentation](https://dtai.cs.kuleuven.be/problog/tutorial/advanced/01_python_interface.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y_4J7fp8ZY9W",
        "outputId": "71047553-a3c2-4e4a-e948-4e05c4e237ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swi-prolog is already the newest version (8.4.2+dfsg-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y swi-prolog\n",
        "! pip install pyswip --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syI41lh1Uw1U"
      },
      "source": [
        " ## Step 1: Define the Logical Rules (Prolog)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRIvghOfLxFL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFbdaf9bLDZ9",
        "outputId": "912fa3b0-12ff-4a65-99c5-548651eabac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'X': 'john'}]\n"
          ]
        }
      ],
      "source": [
        "from pyswip import Prolog\n",
        "\n",
        "prolog = Prolog()\n",
        "\n",
        "prolog.assertz(\"parent(john, alice)\")\n",
        "prolog.assertz(\"parent(alice, bob)\")\n",
        "prolog.assertz(\"grandparent(X, Y) :- parent(X, Z), parent(Z, Y)\")\n",
        "\n",
        "result = list(prolog.query(\"grandparent(X, bob)\"))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaQY4O_FZ-Jq",
        "outputId": "67aa6b15-449a-42a5-d11a-1f9b5abef746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "michael is the father of john\n",
            "michael is the father of gina\n"
          ]
        }
      ],
      "source": [
        "Prolog.assertz(\"father(michael,john)\")\n",
        "Prolog.assertz(\"father(michael,gina)\")\n",
        "list(Prolog.query(\"father(michael,X)\")) == [{'X': 'john'}, {'X': 'gina'}]\n",
        "for soln in Prolog.query(\"father(X,Y)\"):\n",
        "    print(soln[\"X\"], \"is the father of\", soln[\"Y\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaOouKJ5U1cG"
      },
      "source": [
        "## Step 2: Introduce Probabilistic Reasoning (Probabilistic Prolog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t711CCGDeJz9"
      },
      "outputs": [],
      "source": [
        "!pip install problog -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv0ShdPYkFmY",
        "outputId": "8fb1206e-3965-4fec-d090-4c19b055d38f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{grandparent(john,bob): 0.7200000000000001}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from problog.program import PrologString\n",
        "from problog import get_evaluatable\n",
        "\n",
        "problog_code = \"\"\"\n",
        "% Facts with probabilities\n",
        "0.9::parent(john, alice).\n",
        "0.8::parent(alice, bob).\n",
        "grandparent(X, Y) :- parent(X, Z), parent(Z, Y).\n",
        "query(grandparent(X, bob)).\n",
        "\"\"\"\n",
        "\n",
        "get_evaluatable().create_from(problog_code).evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzDN0eloVAlZ"
      },
      "source": [
        "## Step 4: Implement Similarity-Based Reasoning Using Vector Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X_DiwJB0kIPR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "movie_embeddings = {\n",
        "    \"m1\": np.array([0.9, 0.8, 0.1]),\n",
        "    \"m2\": np.array([0.85, 0.75, 0.15]),\n",
        "    \"m3\": np.array([0.2, 0.1, 0.9])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR6PGdWOKwVC",
        "outputId": "9d55ac5e-785a-493b-f0d8-5fc3225f95a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "m1 is similar to m2 [('m2', 0.9988075343833681), ('m3', 0.3123506333062124)]\n",
            "m3 is disimilar from m1 and m2 [('m2', 0.3583550444099566), ('m1', 0.3123506333062124)]\n"
          ]
        }
      ],
      "source": [
        "# define a function that use embedding to find similar alternatives for examples (cos similarity, etc)\n",
        "# integrate this function with prolog\n",
        "\n",
        "def find_similar_movies(movie_name, movie_embeddings, top_n=2):\n",
        "  if movie_name not in movie_embeddings:\n",
        "    return []\n",
        "\n",
        "  movie_vector = movie_embeddings[movie_name].reshape(1, -1)\n",
        "  similarities = []\n",
        "  for other_movie, other_vector in movie_embeddings.items():\n",
        "      if other_movie != movie_name:\n",
        "          similarity = cosine_similarity(movie_vector, other_vector.reshape(1, -1))[0][0]\n",
        "          similarities.append((other_movie, similarity))\n",
        "\n",
        "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "  return similarities[:top_n]\n",
        "\n",
        "\n",
        "similar_movies = find_similar_movies(\"m1\", movie_embeddings)\n",
        "print(\"m1 is similar to m2\", similar_movies)\n",
        "\n",
        "similar_movies = find_similar_movies(\"m3\", movie_embeddings)\n",
        "print(\"m3 is disimilar from m1 and m2\", similar_movies)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WE_fG_3sYJ3q"
      },
      "outputs": [],
      "source": [
        "! pip install fireducks datasets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__2hJEwKvWi"
      },
      "source": [
        "# mention encoder\n",
        "We now know how to develop mention encoders, so we can develop one for movies:\n",
        "*\tTake the list of movie names.\n",
        "*\tFor each name, find mention sentences in a corpus.\n",
        "* Based on these sentences, extract representations using a BERT-family encoder.\n",
        "* Average the mention vectors.\n",
        "* Done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8oaE3LMaN5D",
        "outputId": "af202813-f47f-414b-dd21-42fb7abdb5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deque(['The Shawshank Redemption', 'The Godfather', 'The Dark Knight', 'Pulp Fiction', 'Forrest Gump', 'The Lord of the Rings', 'Inception', 'The Matrix', 'The Silence of the Lambs', 'The Lion King', 'Telnet'])\n"
          ]
        }
      ],
      "source": [
        "from collections import deque\n",
        "\n",
        "# List of movie names\n",
        "movie_names =  deque([\n",
        "    \"The Shawshank Redemption\",\n",
        "    \"The Godfather\",\n",
        "    \"The Dark Knight\",\n",
        "    \"Pulp Fiction\",\n",
        "    \"Forrest Gump\",\n",
        "    \"The Lord of the Rings\",\n",
        "    \"Inception\",\n",
        "    \"The Matrix\",\n",
        "    \"The Silence of the Lambs\",\n",
        "    \"The Lion King\",\n",
        "    \"Telnet\"])\n",
        "\n",
        "\n",
        "print(movie_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttzOY-P-L0_A",
        "outputId": "eb24d7b9-6192-44de-b94d-3cf52a2ba474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DF size before filter (25000, 2)\n",
            "                                                text  label\n",
            "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
            "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
            "2  If only to avoid making this type of film in t...      0\n",
            "3  This film was probably inspired by Godard's Ma...      0\n",
            "4  Oh, brother...after hearing about this ridicul...      0\n",
            "DF size after filter (213, 2)\n",
            "                                                  text  label\n",
            "74   I'm studying Catalan, and was delighted to fin...      0\n",
            "257  Hail Bollywood and men Directors !<br /><br />...      0\n",
            "460  An old intellectual talks about what he consid...      0\n",
            "528  Scarecrow is set in the small American town of...      0\n",
            "751  Note: I couldn't force myself to actually writ...      0\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import fireducks.pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "print(\"DF size before filter\", df.shape)\n",
        "print(df.head(5))\n",
        "\n",
        "# Create a regex pattern that matches any of the items\n",
        "pattern = '|'.join(map(re.escape, movie_names))\n",
        "\n",
        "# Use str.contains to create a boolean mask (na=False avoids errors with NaN)\n",
        "mask = df['text'].str.contains(pattern, na=False)\n",
        "\n",
        "# Filter the DataFrame using the mask\n",
        "filtered_df = df[mask]\n",
        "\n",
        "print(\"DF size after filter\", filtered_df.shape)\n",
        "print(filtered_df.head(5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWV7CnMedSB4",
        "outputId": "96149cb1-5155-4839-d503-2ab2a7ae27b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  text  label  \\\n",
            "74   I'm studying Catalan, and was delighted to fin...      0   \n",
            "257  Hail Bollywood and men Directors !<br /><br />...      0   \n",
            "460  An old intellectual talks about what he consid...      0   \n",
            "528  Scarecrow is set in the small American town of...      0   \n",
            "751  Note: I couldn't force myself to actually writ...      0   \n",
            "\n",
            "                                        cls_embeddings  \n",
            "74   [0.12923839688301086, -0.33946046233177185, 0....  \n",
            "257  [-0.39316117763519287, -0.09070698916912079, 0...  \n",
            "460  [-0.02286955714225769, 0.169071763753891, 0.19...  \n",
            "528  [-0.41711029410362244, -0.07180090248584747, 0...  \n",
            "751  [0.0463026762008667, -0.6328936219215393, 0.39...  \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Specify the BERT model\n",
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the text column\n",
        "inputs = tokenizer(\n",
        "    filtered_df['text'].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract the embeddings for the [CLS] token for each sentence\n",
        "# The [CLS] token is located at index 0 for each sequence\n",
        "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "filtered_df['cls_embeddings'] = cls_embeddings.tolist()\n",
        "\n",
        "print(filtered_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BqTYuJhhICa",
        "outputId": "3cf615f2-2a2d-4907-bac9-85de0bad7f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape torch.Size([213, 768])\n",
            "Average mention vector shape: torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "print(\"Embedding shape\", cls_embeddings.shape)\n",
        "average_vector = torch.mean((cls_embeddings), dim=0)\n",
        "print(\"Average mention vector shape:\", average_vector.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAM9v8nz9hNK"
      },
      "source": [
        "# fine-tune a mention Encoder\n",
        "- you have your mentions of movies\n",
        "- use a contrastive loss to move the mention vectors that are similar (same genre for example) together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PyB2bJP_o3HD"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "class ContrastiveDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text1 = self.df.loc[idx, 'text']\n",
        "        label1 = self.df.loc[idx, 'label']\n",
        "\n",
        "        # Randomly choose another sample (ensuring it's not the same)\n",
        "        other_idx = random.choice([i for i in range(len(self.df)) if i != idx])\n",
        "        text2 = self.df.loc[other_idx, 'text']\n",
        "        label2 = self.df.loc[other_idx, 'label']\n",
        "\n",
        "        # Define the contrastive label: 1 if same rating, 0 otherwise\n",
        "        sentiment_label = 1.0 if label1 == label2 else 0.0\n",
        "        return text1, text2, sentiment_label\n",
        "\n",
        "\n",
        "# Collate function for combining pairs into batches\n",
        "def collate_fn(batch):\n",
        "    text1, text2, sentiment_label = zip(*batch)\n",
        "    return list(text1), list(np.array(text2)), torch.tensor(sentiment_label, dtype=torch.float32)\n",
        "\n",
        "\n",
        "dataset = ContrastiveDataset(filtered_df)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PHp15rX0pR0O"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        # Compute Euclidean distance between two batches of embeddings\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "\n",
        "        # Loss for similar pairs: squared distance;\n",
        "        loss_similar = label * torch.pow(euclidean_distance, 2)\n",
        "\n",
        "        # Loss for dissimilar pairs: squared difference between margin and distance\n",
        "        loss_dissimilar = (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
        "\n",
        "        loss = torch.mean(loss_similar + loss_dissimilar)\n",
        "        return loss\n",
        "\n",
        "criterion = ContrastiveLoss(margin=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2raDeGEooY5Y",
        "outputId": "f4dbae58-e24f-4576-ddeb-0050321e1359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Loss: 2.6969\n",
            "Epoch 2/3, Loss: 0.2768\n",
            "Epoch 3/3, Loss: 0.2419\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# -----------------------------\n",
        "# Training Loop\n",
        "# -----------------------------\n",
        "num_epochs = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for texts1, texts2, labels in dataloader:\n",
        "        # Tokenize both sets of texts\n",
        "        inputs1 = tokenizer(texts1, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        inputs2 = tokenizer(texts2, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Move input tensors to the device (CPU or GPU)\n",
        "        inputs1 = {key: val.to(device) for key, val in inputs1.items()}\n",
        "        inputs2 = {key: val.to(device) for key, val in inputs2.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass: get the outputs and extract the CLS embedding ([CLS] token is at index 0)\n",
        "        outputs1 = model(**inputs1)\n",
        "        outputs2 = model(**inputs2)\n",
        "        cls_embeddings1 = outputs1.last_hidden_state[:, 0, :]\n",
        "        cls_embeddings2 = outputs2.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Compute the contrastive loss using the embeddings and similarity labels\n",
        "        loss = criterion(cls_embeddings1, cls_embeddings2, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ID0f_q9kLQ"
      },
      "source": [
        "# entity encoder\n",
        "We now assume we don't have mentions for movie names. We will devlopp a entity encoder. We can train our enoder using the following prompt. \"Movie has Genre which mean [Mask]\", or simply \"Movie has genre [CLS]\". Train the encoder using a cross-entropy loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evtU8eMw7TbX",
        "outputId": "d7fc8c83-c98c-4f45-9365-76006868c889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      imdbId  tmdbId  movie_id  user_id  rating  \\\n",
            "0  tt2345759  282035      7400    10785     4.0   \n",
            "1  tt2278388  120467       545    41527     3.0   \n",
            "2  tt4882376  433247      7738    12551     3.0   \n",
            "3  tt6966692  490132     12323    28576     4.5   \n",
            "4  tt2436386  227719      1546    10178     3.5   \n",
            "\n",
            "                                title  \\\n",
            "0                    The Mummy (2017)   \n",
            "1    Grand Budapest Hotel, The (2014)   \n",
            "2  First They Killed My Father (2017)   \n",
            "3                   Green Book (2018)   \n",
            "4              Project Almanac (2015)   \n",
            "\n",
            "                                     genres  \\\n",
            "0  Action|Adventure|Fantasy|Horror|Thriller   \n",
            "1                              Comedy|Drama   \n",
            "2                                     Drama   \n",
            "3                              Comedy|Drama   \n",
            "4                           Sci-Fi|Thriller   \n",
            "\n",
            "                                             posters  \n",
            "0  https://m.media-amazon.com/images/M/MV5BMTkwMT...  \n",
            "1  https://m.media-amazon.com/images/M/MV5BMzM5Nj...  \n",
            "2  https://m.media-amazon.com/images/M/MV5BMTgzOT...  \n",
            "3  https://m.media-amazon.com/images/M/MV5BYzIzYm...  \n",
            "4  https://m.media-amazon.com/images/M/MV5BMTUxMj...  \n"
          ]
        }
      ],
      "source": [
        "# Load the Movielens dataset\n",
        "dataset = load_dataset(\"ashraq/movielens_ratings\", split=\"train\")\n",
        "\n",
        "# Randomly select 200 examples\n",
        "sampled_dataset = dataset.shuffle(seed=42).select(range(200))\n",
        "\n",
        "sampled_df = pd.DataFrame(sampled_dataset)\n",
        "\n",
        "print(sampled_df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1uJEs8XF7y0j"
      },
      "outputs": [],
      "source": [
        "class EntityDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=32):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get first genre from split by \"|\"\n",
        "        genre_word = self.df.loc[idx, 'genres'].split(\"|\")[0]\n",
        "\n",
        "        # Construct our prompt\n",
        "        prompt = f\"Movie has Genre which mean {self.tokenizer.mask_token}\"\n",
        "\n",
        "        # Tokenize the prompt text\n",
        "        encoding = self.tokenizer(prompt,\n",
        "                                  max_length=self.max_length,\n",
        "                                  truncation=True,\n",
        "                                  return_tensors=\"pt\")\n",
        "        # Remove the batch dimension\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Create labels: All tokens are ignored except the one corresponding to the mask\n",
        "        # All other positions are set to -100\n",
        "        labels = torch.full(input_ids.shape, -100)\n",
        "\n",
        "        # Find the index of the mask token\n",
        "        mask_positions = (input_ids == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "        if len(mask_positions[0]) == 0:\n",
        "            raise ValueError(\"No mask token found in input_ids!\")\n",
        "        mask_index = mask_positions[0].item()  # assume only one mask token in our prompt\n",
        "\n",
        "        # Convert the genre word to its token id.\n",
        "        # Ensure that the genre word is in the tokenizer's vocabulary as a single token\n",
        "        target_token_id = self.tokenizer.convert_tokens_to_ids(genre_word)\n",
        "        labels[mask_index] = target_token_id\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item[\"input_ids\"] for item in batch]\n",
        "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_masks,\n",
        "        \"labels\": labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y_MZNeH9kkJ",
        "outputId": "75b669be-3ee3-478e-e1e4-7e3f4088179d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Loss: 0.2319\n",
            "Epoch 2/3 - Loss: 0.0000\n",
            "Epoch 3/3 - Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "\n",
        "df = pd.DataFrame(sampled_df)\n",
        "\n",
        "# We use BertForMaskedLM which automatically computes\n",
        "# cross-entropy loss over the masked position when given labels.\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "dataset = EntityDataset(df, tokenizer, max_length=32)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # set model to training mode\n",
        "    total_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        # Move tensors to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass - the model will compute loss if labels are provided\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass and optimizer update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XQTHOawASVl"
      },
      "source": [
        "# Joint encoder on logical formulas\n",
        "- consider a set of rules\n",
        "- form pairs of (premises, hypothesis) pairs where there are entailment\n",
        "- train a joint encoder with [CLS] p_1, p_2, ...,p_n [Sep]h[sep]\n",
        "- use binary cross-enropy\n",
        "\n",
        "to predict entailment, feed [CLS] to a linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "I76fjNrcAhm8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
