{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Byte Pair Encoding (BPE) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Retrieve Text from url\n",
    "url = \"https://olki.loria.fr/cerisara/lexres/nh.txt\"\n",
    "\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read() \n",
    "text = data.decode('utf-8')\n",
    "\n",
    "# Save text to file\n",
    "with open('nh.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text from file\n",
    "with open('nh.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10246792\n",
      "[61, 95, 61, 95, 32, 70, 105, 108, 101, 58, 87, 105, 107, 105, 46, 112, 110, 103, 10, 84, 104, 105, 115, 32, 102, 97, 118, 105, 99, 111, 110, 32, 115, 104, 111, 119, 115, 32, 97, 32, 32, 102, 114, 111, 109, 32, 78, 101, 116, 72]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = text.encode('utf-8')\n",
    "\n",
    "# Convert text to list of integers\n",
    "tokens = list(map(int, tokens))\n",
    "\n",
    "# print length of tokens\n",
    "print(len(tokens))\n",
    "\n",
    "# print first 50 Unigram tokens\n",
    "print(tokens[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(tokens):\n",
    "    counts = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        # pair is a tuple of two integers\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(tokens, pair, new_token):\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge (101, 32) -> 241\n",
      "merge (32, 116) -> 242\n",
      "merge (32, 97) -> 243\n",
      "merge (105, 110) -> 244\n",
      "merge (101, 114) -> 245\n",
      "merge (115, 32) -> 246\n",
      "merge (242, 104) -> 247\n",
      "merge (111, 110) -> 248\n",
      "merge (116, 32) -> 249\n",
      "merge (111, 117) -> 250\n",
      "merge (100, 32) -> 251\n",
      "merge (97, 110) -> 252\n",
      "merge (111, 114) -> 253\n",
      "merge (247, 241) -> 254\n",
      "merge (61, 95) -> 255\n",
      "merge (101, 110) -> 256\n",
      "merge (115, 116) -> 257\n",
      "merge (44, 32) -> 258\n",
      "merge (108, 101) -> 259\n",
      "merge (105, 116) -> 260\n",
      "merge (114, 101) -> 261\n",
      "merge (32, 32) -> 262\n",
      "merge (244, 103) -> 263\n",
      "merge (111, 32) -> 264\n",
      "merge (121, 32) -> 265\n",
      "merge (97, 116) -> 266\n",
      "merge (97, 114) -> 267\n",
      "merge (108, 108) -> 268\n",
      "merge (111, 102) -> 269\n",
      "merge (46, 32) -> 270\n",
      "merge (97, 99) -> 271\n",
      "merge (243, 110) -> 272\n",
      "merge (243, 32) -> 273\n",
      "merge (99, 101) -> 274\n",
      "merge (101, 116) -> 275\n",
      "merge (105, 99) -> 276\n",
      "merge (114, 111) -> 277\n",
      "merge (255, 255) -> 278\n",
      "merge (278, 32) -> 279\n",
      "merge (10, 279) -> 280\n",
      "merge (250, 114) -> 281\n",
      "merge (97, 108) -> 282\n",
      "merge (105, 115) -> 283\n",
      "merge (245, 32) -> 284\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 300\n",
    "num_merges = vocab_size - 256\n",
    "\n",
    "tokens_clone = list(tokens)\n",
    "\n",
    "merges = {} # map from pair to new token\n",
    "print(f\"num_merges = {num_merges}\")\n",
    "for i in range(num_merges):\n",
    "    counts = get_statistics(tokens_clone)\n",
    "    pair = max(counts, key=counts.get)\n",
    "    new_token = max(tokens_clone) + 1\n",
    "    print(f\"merge {pair} -> {new_token}\")\n",
    "    tokens_clone = merge(tokens_clone, pair, new_token)\n",
    "    merges[pair] = new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length = 10246792\n",
      "tokens_clone length = 7342917\n",
      "compression rate = 1.40\n"
     ]
    }
   ],
   "source": [
    "print(f\"tokens length = {len(tokens)}\")\n",
    "print(f\"tokens_clone length = {len(tokens_clone)}\")\n",
    "print(f\"compression rate = {len(tokens) / len(tokens_clone):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 231\n",
      "merges = {(101, 32): 241, (32, 116): 242, (32, 97): 243, (105, 110): 244, (101, 114): 245, (115, 32): 246, (242, 104): 247, (111, 110): 248, (116, 32): 249, (111, 117): 250, (100, 32): 251, (97, 110): 252, (111, 114): 253, (247, 241): 254, (61, 95): 255, (101, 110): 256, (115, 116): 257, (44, 32): 258, (108, 101): 259, (105, 116): 260, (114, 101): 261, (32, 32): 262, (244, 103): 263, (111, 32): 264, (121, 32): 265, (97, 116): 266, (97, 114): 267, (108, 108): 268, (111, 102): 269, (46, 32): 270, (97, 99): 271, (243, 110): 272, (243, 32): 273, (99, 101): 274, (101, 116): 275, (105, 99): 276, (114, 111): 277, (255, 255): 278, (278, 32): 279, (10, 279): 280, (250, 114): 281, (97, 108): 282, (105, 115): 283, (245, 32): 284}\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocab_size = {len(set(tokens_clone))}\")\n",
    "print(f\"merges = {merges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
